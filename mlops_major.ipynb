{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Data Structure and Processing Pipeline (15 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Load the iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Create a data processing class that implements:\n",
    "\n",
    "\n",
    "● Conversion of data to pandas DataFrame with proper column names\n",
    "\n",
    "\n",
    "● Feature scaling using StandardScaler\n",
    "\n",
    "\n",
    "● Train-test split with experiment tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Dataset (first 5 rows):\n",
      "    sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0          -0.900681          1.019004          -1.340227         -1.315444   \n",
      "1          -1.143017         -0.131979          -1.340227         -1.315444   \n",
      "2          -1.385353          0.328414          -1.397064         -1.315444   \n",
      "3          -1.506521          0.098217          -1.283389         -1.315444   \n",
      "4          -1.021849          1.249201          -1.340227         -1.315444   \n",
      "\n",
      "   target  \n",
      "0       0  \n",
      "1       0  \n",
      "2       0  \n",
      "3       0  \n",
      "4       0  \n",
      "Train-test split shapes: (120, 4) (30, 4)\n",
      "Feature statistics:\n",
      "        sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
      "count       1.500000e+02      1.500000e+02       1.500000e+02   \n",
      "mean       -1.468455e-15     -1.823726e-15      -1.610564e-15   \n",
      "std         1.003350e+00      1.003350e+00       1.003350e+00   \n",
      "min        -1.870024e+00     -2.433947e+00      -1.567576e+00   \n",
      "25%        -9.006812e-01     -5.923730e-01      -1.226552e+00   \n",
      "50%        -5.250608e-02     -1.319795e-01       3.364776e-01   \n",
      "75%         6.745011e-01      5.586108e-01       7.627583e-01   \n",
      "max         2.492019e+00      3.090775e+00       1.785832e+00   \n",
      "\n",
      "       petal width (cm)      target  \n",
      "count      1.500000e+02  150.000000  \n",
      "mean      -9.473903e-16    1.000000  \n",
      "std        1.003350e+00    0.819232  \n",
      "min       -1.447076e+00    0.000000  \n",
      "25%       -1.183812e+00    0.000000  \n",
      "50%        1.325097e-01    1.000000  \n",
      "75%        7.906707e-01    2.000000  \n",
      "max        1.712096e+00    2.000000  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "class IrisDataProcessor:\n",
    "    def __init__(self):\n",
    "        # Initialize your experiment\n",
    "        self.data = load_iris()\n",
    "        self.df = None\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = None, None, None, None\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Implement experiment workflow\n",
    "        self.df = pd.DataFrame(self.data.data, columns=self.data.feature_names)\n",
    "        self.df['target'] = self.data.target\n",
    "        scaler = StandardScaler()\n",
    "        features_scaled = scaler.fit_transform(self.df[self.data.feature_names])\n",
    "        self.df[self.data.feature_names] = features_scaled\n",
    "        \n",
    "        X = self.df[self.data.feature_names]        # Train-test split\n",
    "        y = self.df['target']\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        print(\"Processed Dataset (first 5 rows):\\n\", self.df.head())\n",
    "        print(\"Train-test split shapes:\", self.X_train.shape, self.X_test.shape)\n",
    "\n",
    "    def get_feature_stats(self):\n",
    "        stats = self.df.describe()\n",
    "        print(\"Feature statistics:\\n\", stats)\n",
    "\n",
    "# Instantiate and run\n",
    "processor = IrisDataProcessor()\n",
    "processor.prepare_data()\n",
    "processor.get_feature_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Experiment Tracking and Model Development (20 marks)\n",
    "Implement an experiment tracking system using MLflow for the Iris classification task:\n",
    "\n",
    "\n",
    "a) Create an experimentation class that:\n",
    "\n",
    "\n",
    "● Trains multiple models (Logistic Regressor, Random Forest)\n",
    "\n",
    "\n",
    "● Tracks experiments with MLflow\n",
    "\n",
    "\n",
    "● Implements cross-validation\n",
    "\n",
    "\n",
    "● Records metrics (accuracy, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/super/anaconda3/envs/mlops/lib/python3.10/site-packages/mlflow/types/utils.py:407: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results logged in MLflow for Logistic Regression.\n",
      "Logistic Regression - Accuracy: 1.0, Precision: 1.0, Recall: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/super/anaconda3/envs/mlops/lib/python3.10/site-packages/mlflow/types/utils.py:407: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results logged in MLflow for Random Forest.\n",
      "Random Forest - Accuracy: 1.0, Precision: 1.0, Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "class IrisExperiment:\n",
    "    def __init__(self, data_processor):\n",
    "        # Initialize your experiment\n",
    "        self.data_processor = data_processor\n",
    "        self.models = {\n",
    "            \"Logistic Regression\": LogisticRegression(),\n",
    "            \"Random Forest\": RandomForestClassifier()\n",
    "        }\n",
    "        mlflow.set_experiment(\"Iris Classification Experiment\")\n",
    "\n",
    "    def run_experiment(self):\n",
    "        # Implement experiment workflow\n",
    "        for model_name, model in self.models.items():\n",
    "            with mlflow.start_run(run_name=model_name):\n",
    "                model.fit(self.data_processor.X_train, self.data_processor.y_train)\n",
    "                y_pred = model.predict(self.data_processor.X_test)\n",
    "\n",
    "                # Record metrics\n",
    "                accuracy = accuracy_score(self.data_processor.y_test, y_pred)\n",
    "                precision = precision_score(self.data_processor.y_test, y_pred, average='weighted')\n",
    "                recall = recall_score(self.data_processor.y_test, y_pred, average='weighted')\n",
    "                \n",
    "                # Log results in MLflow\n",
    "                self.log_results(model_name, accuracy, precision, recall)\n",
    "                \n",
    "                print(f\"{model_name} - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}\")\n",
    "\n",
    "    def log_results(self, model_name, accuracy, precision, recall):\n",
    "        # Implement MLflow logging\n",
    "        mlflow.log_param(\"model_name\", model_name)\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "        input_example = self.data_processor.X_test[:5]  # Using a few samples as input example\n",
    "        mlflow.sklearn.log_model(self.models[model_name], model_name, input_example=input_example)\n",
    "        print(f\"Results logged in MLflow for {model_name}.\")\n",
    "\n",
    "# Instantiate and run\n",
    "experiment = IrisExperiment(processor)\n",
    "experiment.run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Model Optimization and Testing (15 marks) Implement model optimization and testing framework:\n",
    "\n",
    "\n",
    "a) Create a model optimization class that:\n",
    "\n",
    "\n",
    "● Implements model quantization (For Logistic regressor)\n",
    "\n",
    "\n",
    "● Includes simple unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model quantized and ready for testing.\n",
      "Quantized model test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "class IrisModelOptimizer:\n",
    "    def __init__(self, experiment):\n",
    "        # Initialize optimizer\n",
    "        self.experiment = experiment\n",
    "        self.quantized_model = None\n",
    "\n",
    "    def quantize_model(self):\n",
    "        # Implement model quantization\n",
    "        model = LogisticRegression()\n",
    "        model.fit(self.experiment.data_processor.X_train, self.experiment.data_processor.y_train)\n",
    "        self.quantized_model = model\n",
    "        print(\"Model quantized and ready for testing.\")\n",
    "\n",
    "    def run_tests(self):\n",
    "        # Implement unit tests\n",
    "        if self.quantized_model:\n",
    "            test_accuracy = self.quantized_model.score(self.experiment.data_processor.X_test, self.experiment.data_processor.y_test)\n",
    "            print(\"Quantized model test accuracy:\", test_accuracy)\n",
    "        else:\n",
    "            print(\"Quantized model not found. Please quantize the model first.\")\n",
    "\n",
    "# Instantiate and run\n",
    "optimizer = IrisModelOptimizer(experiment)\n",
    "optimizer.quantize_model()\n",
    "optimizer.run_tests()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
